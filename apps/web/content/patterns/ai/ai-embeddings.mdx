# AI Embeddings

Vector embeddings for semantic search and RAG applications.

## The Problem

Building search and RAG (Retrieval-Augmented Generation) requires:

- Converting text to vector embeddings
- Storing and querying vectors efficiently
- Handling long documents with chunking
- Semantic similarity search

## The Solution

Use OpenAI's text-embedding-3-small model with Upstash Vector for serverless vector storage. Includes chunking utilities for long documents.

## Files

```typescript title="lib/ai/embeddings.ts"
import { embed, embedMany } from "ai";
import { openai } from "@ai-sdk/openai";

const embeddingModel = openai.embedding("text-embedding-3-small");

export async function generateEmbedding(text: string): Promise<number[]> {
  const { embedding } = await embed({
    model: embeddingModel,
    value: text,
  });
  return embedding;
}

export async function generateEmbeddings(texts: string[]): Promise<number[][]> {
  const { embeddings } = await embedMany({
    model: embeddingModel,
    values: texts,
  });
  return embeddings;
}

export function chunkText(
  text: string,
  options: { maxChunkSize?: number; overlap?: number } = {}
): string[] {
  const { maxChunkSize = 1000, overlap = 200 } = options;
  const chunks: string[] = [];
  const paragraphs = text.split(/\n\n+/);
  let currentChunk = "";

  for (const paragraph of paragraphs) {
    if ((currentChunk + paragraph).length <= maxChunkSize) {
      currentChunk += (currentChunk ? "\n\n" : "") + paragraph;
    } else {
      if (currentChunk) {
        chunks.push(currentChunk);
        const words = currentChunk.split(" ");
        currentChunk = words.slice(-Math.floor(overlap / 5)).join(" ");
      }
      currentChunk += "\n\n" + paragraph;
    }
  }

  if (currentChunk) chunks.push(currentChunk);
  return chunks;
}
```

```typescript title="lib/ai/vector-store.ts"
import { Index } from "@upstash/vector";
import { generateEmbedding, generateEmbeddings } from "./embeddings";

const index = new Index({
  url: process.env.UPSTASH_VECTOR_REST_URL!,
  token: process.env.UPSTASH_VECTOR_REST_TOKEN!,
});

export interface VectorDocument {
  id: string;
  content: string;
  metadata?: Record<string, unknown>;
}

export async function upsertDocument(doc: VectorDocument): Promise<void> {
  const embedding = await generateEmbedding(doc.content);
  await index.upsert({
    id: doc.id,
    vector: embedding,
    metadata: { content: doc.content, ...doc.metadata },
  });
}

export async function search(
  query: string,
  options: { topK?: number } = {}
): Promise<{ id: string; score: number; content: string }[]> {
  const { topK = 5 } = options;
  const queryEmbedding = await generateEmbedding(query);

  const results = await index.query({
    vector: queryEmbedding,
    topK,
    includeMetadata: true,
  });

  return results.map((r) => ({
    id: r.id as string,
    score: r.score,
    content: (r.metadata?.content as string) || "",
  }));
}
```

```typescript title="lib/ai/rag.ts"
import { generateText } from "ai";
import { getModel } from "./providers";
import { search } from "./vector-store";

export async function ragGenerate(query: string, options: { topK?: number } = {}) {
  const results = await search(query, options);

  const context = results.map((r, i) => `[${i + 1}] ${r.content}`).join("\n\n");

  const systemPrompt = `Use the following context to answer the question.
If the context doesn't contain relevant information, say so.

Context:
${context}`;

  const { text } = await generateText({
    model: getModel("default"),
    system: systemPrompt,
    prompt: query,
  });

  return { answer: text, sources: results };
}
```

## Configuration

### Environment Variables

| Variable                    | Description                   | Required |
| --------------------------- | ----------------------------- | -------- |
| `OPENAI_API_KEY`            | OpenAI API key for embeddings | Yes      |
| `UPSTASH_VECTOR_REST_URL`   | Upstash Vector URL            | Yes      |
| `UPSTASH_VECTOR_REST_TOKEN` | Upstash Vector token          | Yes      |

## Usage

### Index Documents

```typescript
import { upsertDocument, chunkText } from "@/lib/ai/vector-store";

const document = await fetchDocument();
const chunks = chunkText(document.content);

for (let i = 0; i < chunks.length; i++) {
  await upsertDocument({
    id: `${document.id}-chunk-${i}`,
    content: chunks[i],
    metadata: { documentId: document.id },
  });
}
```

### Semantic Search

```typescript
import { search } from "@/lib/ai/vector-store";

const results = await search("How do I configure the database?", { topK: 5 });
```

### RAG Query

```typescript
import { ragGenerate } from "@/lib/ai/rag";

const { answer, sources } = await ragGenerate("What are the best practices?");
```

## Alternatives

- **Pinecone** - Industry-leading vector database
- **Weaviate** - Open-source with GraphQL API
- **Chroma** - Open-source, local-first
- **pgvector** - Vector search for PostgreSQL
